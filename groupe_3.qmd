---
title: "Groupe 3"
autor: "Gabriel-Marie Arnault, Giuliana Brambilla, Apolline Byrdy, Anaelle Hulbert, Manon Le Bihan, Kilian Prevost, Chloé Van der Swaelmen et Léa Vuille"
source : "Yannick Outreman, selon les données de Gotelli & Ellison (2002) - Biogeography at a regional scale : determinants of antspecies density in New England bogs and forests. Ecology, 83 : 1604-1609."
editor: visual
bibliography: references.bib
---

# INTRODUCTION


-   **1.Article presentation**

Our data come from the article "Biogeography at a Regional Scale: Determinants of Ant Spec Density in New England Bogs and Forests" by Gotelli & Ellison published in *Ecology* in 2002 (83(6), 2002, pp. 1604--1609) (https://doi.org/10.1890/0012-9658(2002)083\[1604:BAARSD\]2.0.CO;2)

-   **2.Biological context**


Gotelli and Ellison are two ecologists, the first specializing in questions of organization of animal and plant communities, the second in the disintegration and reassembly of ecosystems following natural and anthropogenic disturbances. Here, they were interested in the influence of the latitudinal gradient on the species richness of ants in the state of New England in the northeastern United States. Their study was carried out on 22 sites where, for each, 25 traps were placed in an ombrotrophic peat bog and 25 in the surrounding forest. The species of ants contained in the traps are subsequently identified in the laboratory and, for a given site and habitat, they calculated the species richness (total number of species present).


-   **3.Datas and variables**


The variables used are :\
- *Site* : the name of the sampled site\
- *Latitude* : site latitude (quantitative)\
- *Altitude* : site altitude (quantitative)\
- *Area* : the area of the peat bog of the site (quantitative)\
- *Location* : the location of the traps ("Bog" or "Forest") (qualitative)\
- *Nsp* : the specific richness of ants, corresponding to the response variable


-   **4.Research question**

Of the three questions that Gotelli and Ellison address in this study, we focus here on the first two:

1\) Is species density correlated with latitude over such a narrow latitudinal range?

2\) Does the latitudinal gradient persist after statistically controlling for site differences in vegetation composition?

In other words: what are the quantitative (latitude, altitude, area) and qualitative (location) variables that determine ant species richness in New England peatlands?


-   **5.Importing the dataset**


From then on, we can import the dataset located in file "BogAnts.txt"

```{r Importing dataset, echo=TRUE, message=FALSE, include=TRUE}

# Session Configuration :
rm(list=ls()) # Cleaning the work environment

# Importation
dataAnts <- read.table("BogAnts.txt", dec=".", header = TRUE)

# Transforming location into a factor
dataAnts$Location<-as.factor(dataAnts$Location)

# Checking the type of variables
str(dataAnts)

```

[**Conclusion**]{.underline} **:**

Qualitative or quantitative variables are well considered as such.

-   **6.Which model and why?**

To model the relationship between a quantitative response variable $Y$ and explanatory variables $X_{1}$,$X_{2}$...$X_{p}$, quantitative and/or qualitative we use general linear models, or *glm*, (linear regression, ANCOVA, ANOVA). To be able to use these *glm*, three conditions of application are necessary: ​​*homogeneity of variances*, *residual independence* and *residual normality*. However, the $Y$ responses analyzed in ecology can sometimes be discrete and deviate from the hypothesis of normality.

For discrete response variables (counts, binary data), the variance of $Y$ generally varies with the mean of $Y$. This relationship then seems to indicate that the variance cannot be homogeneous in a data set, due to its dependence on the mean. However, general linear models have a constant variance as their application hypothesis (i.e. a homogeneity of the variance). Furthermore, when applying a general linear model to count data, the predicted values ​​obtained may be negative (which is impossible given the nature of $Y$) and the residuals may not be normally distributed. Therefore, using a general linear model (*glm*) on discrete responses very generally generates deviations from the application conditions. In conclusion, it becomes necessary to use a method more suited to the analysis of discrete response variables such as generalized linear models (*glim*).


**Here are the steps to follow to establish a generalized linear model:**\
1. Formulate the hypothesis of the distribution law of the response variable $Y_{i}$\
2. Explore the data (outliers, distribution of values,)\
3. Analyze the interactions between the variables Y and Xs (analysis of the relationships between X and Y, between Xs, and search for collinearity between Xs)\
4. Proceed with statistical analyses (construction of the model, analysis of the coefficients, explanatory power of the model)\
5. Conclude on the results

A generalized linear model is written as follows (equation 1): $$g(\mu_{y})= \alpha+ \beta_{1}.X_{i1}+ \beta_{2}.X_{i2}+\beta_{3}.X_{i3}+...\beta_{p}.X_{ip} = \eta $$ {#eq-1} The linear predictor, $\eta$, is derived from the linear model as the sum of the $p$ terms associated with each of the $\beta_{p}$ parameters. Its value is obtained by transforming the value of $Y$ by the link function, and the predicted value of $Y$ is obtained by applying to $\eta$ the inverse function of the link function.


To illustrate this point, we are interested in counts. A count is a discrete and positive random variable. It is possible that the distribution of its values follows a Poisson distribution, a Negative Binomial distribution or even a Normal distribution when the values are very large.

Our example illustrates a distribution following the Poisson law.

The Poisson law is useful for describing rare and independent discrete events. Thus, a variable $Y$ that follows a Poisson law with parameter $\lambda$ is written as follows (equation 2): $$Pr(Y=y)=\frac{e^{-\lambda}.\ lambda^y}{y!}$$ {#eq-2} where $y$ represents the observed number of occurrences (i.e., $y$=0, 1, 2...} and $\lambda$ the mean and variance of the Poisson law (i.e. in the Poisson law: E($y$)= Var($y$)=$\lambda$).

In a Poisson-type *glim*, the link function is the **log** function. Thus, the model is written (equation 3): $$log(\mu_{y})= \alpha+ \beta_{1}.X{i1}+ \beta_{2}.X{i2}+\beta_{3}.X{i3}+...\beta_{p}.X{ip} = \eta $$ {#eq-3} 150 / 5 000 The predicted values ​​of $Y$ are obtained by applying to $\eta$ the inverse function of the link function, here the **log** function (equation 4): $$\mu_{y}= e^{\alpha+ \beta_{1}.X{i1}+ \beta_{2}.X{i2}+\beta_{3}.X{i3}+...\beta_{p}.X{ip}} = e^{\eta} $$ {#eq-4}

Generalized linear models of the Poisson type have an error structure that follows a Poisson distribution. This structure allows, among other things, to precisely define the relationship between the mean and the variance. This relationship is exploited by the maximum likelihood method to estimate the coefficients and standard errors of the parameters of the GLM model.


-   **7.Environment Configuration and Packages**

To analyze our data we set up the session by cleaning the workspace. We will also need to load the following packages : *knitr*, *ggplot2*, *tinytext*, *corrplot*, *plot3D*, *DHARMa*, *rcompanion*, *lattice*, *patchwork*, *MASS* et *rcompanion*

```{r init, include=FALSE, message=FALSE, warning=FALSE}

# Loading of R packages:
library(knitr)
library(ggplot2) # Package for better plot visuals
library(tinytex) # For pdf output
library(corrplot) # Correlation matrix calculation
library(plot3D) # For 3D plots
library(DHARMa) # Model diagnosis
library(rcompanion) # Pseudo R² model
library(lattice) # Multipannel plots
library(patchwork) # Display multiple ggplots on one page
library(MASS) # Functions and datasets for analyses

```

-   **8. Checking for missing data**

```{r datalack, echo=TRUE,include=TRUE}

# Checking for missing data
colSums(is.na(dataAnts))
summary(dataAnts)

```

There are no missing values.

-   **9. Analysis process**


From this, we will first examine the dataset (outliers, potential relationships between Y and Xs, ...) in order to have a first impression. Subsequently, we will carry out our statistical analysis by building and refining our model. Finally, we will analyze its coefficients and its explanatory power in order to validate it or not.

# EXPLORATION DU JEU DE DONNEES

    1.  **Présentation du jeu de données**

    **a. Valeurs aberrantes dans** $Y$ et distribution des valeurs de $Y$


La première étape de l'exploration du jeu de données consiste à vérifier la présence de potentielles valeurs aberrantes dans la variable réponse $Y$ et à examiner la distribution de ses valeurs.

**Valeurs aberrantes :** dans un jeu de données écologiques, il est probable de rencontrer des individus présentant des valeurs extrêmes pour la variable réponse. Ces individus, en raison de leurs valeurs inhabituelles, peuvent avoir un impact disproportionné sur la construction du modèle. Toutefois, ces valeurs extrêmes peuvent résulter soit de la variabilité naturelle des phénomènes écologiques, soit d'erreurs (par exemple, des erreurs de mesure ou de manipulation). Identifier ces individus avant l'analyse statistique permet de prévenir les erreurs et de justifier, si nécessaire, l'exclusion de certaines valeurs aberrantes pour améliorer la robustesse du modèle.

Les valeurs aberrantes peuvent être identifiées grâce à un **Boxplot** ou à un **Cleveland plot** des valeurs de $Y$.

```{r valeurs aberrantes de Y, echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}

# Boxplot 
bp <- ggplot(data = dataAnts, aes(y = Nsp))+
  geom_boxplot()+
  labs(subtitle = "A. Boxplot",
       y = "richesse spécifique")+
  theme(
    panel.background = element_rect(fill="white"),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    
    axis.title.x = element_text(size = 8, color = "darkblue"),
    axis.title.y = element_text(size = 8, color = "darkblue"),
    plot.title = element_text(size = 14, face = "bold", color = "black"),       
    plot.subtitle = element_text(size = 10, color = "black"),                  
    plot.caption = element_text(size = 4, face = "italic", color = "darkgrey")
  )

# Cleveland plot 
cp <- ggplot(data = dataAnts, aes(x = Nsp, y = 1:nrow(dataAnts)))+
  geom_jitter(width = 0.1, size = 3, color = "black")+
  labs(subtitle = "B. Cleveland plot",
       x = "richesse spécifique")+
  theme_minimal()+
  theme(
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    axis.title.y = element_blank(),
    
    axis.title.x = element_text(size = 8, color = "darkblue"),
    plot.title = element_text(size = 14, face = "bold", color = "black"),       
    plot.subtitle = element_text(size = 10, color = "black"),                  
    plot.caption = element_text(size = 4, face = "italic", color = "darkgrey")
  )

## Affichage 
(bp + cp) + 
  plot_layout(widths = c(1, 2)) + 
  plot_annotation(title = "Figure 1 : Analyse des valeurs aberrantes de Y")

```

D'après la figure 1, $Y$ ne présente pas de valeurs extrêmes ou aberrantes.

**Distribution des valeurs :** analyser la distribution des valeurs de $Y$ permet d'identifier *a priori* la loi de probabilité qui expliquera au mieux les processus écologiques en cours. Cela est d'autant plus intéressant si les données suggèrent une relation non linéaire ou asymétrique.

La distribution des valeurs de $Y$ peut être visualisée grâce à un **histogramme** ou à un **Normal Q-Q plot**.

```{r distribution de Y, echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}

# Histogramme 
hg <- ggplot(data = dataAnts, aes(x = Nsp))+
  geom_histogram(binwidth = 2, fill = "blue", color = "black")+
  theme_minimal()+
  labs(subtitle = "A. Histogramme",
       x = "richesse spécifique",
       y = "fréquence")+
  theme(
    axis.title.x = element_text(size = 8, color = "darkblue"),
    axis.title.y = element_text(size = 8, color = "darkblue"),
    plot.title = element_text(size = 14, face = "bold", color = "black"),       
    plot.subtitle = element_text(size = 10, color = "black"),                  
    plot.caption = element_text(size = 4, face = "italic", color = "darkgrey")
    )

qqp <- ggplot(data = dataAnts, aes(sample = Nsp))+
  geom_qq_line(color = "red", size = 1.2)+
  geom_qq()+
  theme_minimal()+
  labs(subtitle = "B. Q-Q plot",
       x = "quantiles théoriques",
       y = "quantiles observés")+
  theme(
    axis.title.x = element_text(size = 8, color = "darkblue"),
    axis.title.y = element_text(size = 8, color = "darkblue"),
    plot.title = element_text(size = 14, face = "bold", color = "black"),       
    plot.subtitle = element_text(size = 10, color = "black"),                  
    plot.caption = element_text(size = 4, face = "italic", color = "darkgrey")
  )

# Affichage
(hg + qqp) +
  plot_layout(widths = c(1, 2)) + 
  plot_annotation(title = "Figure 2 : Analyse de la distribution de Y")

```

La distribution de $Y$ (exponentielle négative) et la nature du jeu de données (comptage sans aggrégation) suggèrent que $Y$ suit une loi de Poisson. Cette hypothèse est appuyée par l'écart à la normalité des valeurs de $Y$ observé en figure 2B.

[**Conclusion**]{.underline} **:**

Après analyse des valeurs aberrantes et de la distribution de la variable réponse $Y$, il semble pertinent d'utiliser $Y$ tel quel en appliquant une loi de Poisson.


**b.** $X$ quantitative : présence de valeurs abérrantes et distribution de valeurs de $X$


Le jeu de données dispose de trois variables explicatives quantitatives : *Latitude*, *Elevation* et *Area.* De même que pour la variable réponse Y, il est nécessaire d'observer la présence de valeurs aberrantes ainsi que la distribution de chacune des variables réponses.

[**La latitude**]{.underline} **:**

```{r Latitude, echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}

p1 <- ggplot(dataAnts) +
  aes(x=Latitude, y=factor(1:nrow(dataAnts), levels = 1:nrow(dataAnts))) + # Affichage avec classification
  geom_jitter(width=0, height=0.1) +
  scale_x_continuous(breaks = seq(41.5, 45, by = 0.5)) + # Valeurs de l'axe X
  labs(title = "Figure 3 : Variable explicative Latitude",
       subtitle = "A. Cleveland Plot",
       x= "Latitude") +  # Titres
  theme_minimal() +
  theme(axis.text.y = element_blank(),   # Supprimer les étiquettes de l'axe Y
        axis.ticks.y = element_blank(),  # Supprimer les ticks de l'axe Y
        axis.title.y = element_blank(),  # Supprimer le titre de l'axe Y
        axis.title.x = element_text(size = 8, color = "darkblue"),
        plot.title = element_text(size = 14, face = "bold", color = "black"),
        plot.subtitle = element_text(size = 10, color = "black")
        )   

p2 <- ggplot(dataAnts) +
  aes(x = Latitude) +  # Utiliser Latitude sur l'axe X
  geom_histogram(binwidth = 0.25, fill = "blue", color = "black", alpha = 0.7) +  # Histogramme
  scale_x_continuous(breaks = seq(41.5, 45, by = 0.5)) +  # Valeurs de l'axe X
  scale_y_continuous(breaks = seq(0, 10, by = 2)) +  # Valeurs de l'axe X
  labs(subtitle = "B. Histogramme",
       x = "Latitude") +  # Titres
  theme_minimal() +  # Appliquer un thème minimal
  theme(
    axis.title.x = element_text(size = 8, color = "darkblue"), 
    axis.title.y = element_text(size = 8, color = "darkblue"),
    plot.subtitle = element_text(size = 10, color = "black")
  ) 

p3 <- ggplot(dataAnts, aes(sample = Latitude)) +
  labs(subtitle = "C. Quantile-Quantile Plot",
       caption = "D'après le jeu de données dataAnts",
       x = "Quantiles Théorique",
       y = "Quantiles Observés") +
  stat_qq() +  # Ajoute les points du Q-Q plot
  stat_qq_line(col = "red", size = 1) +  # Ajoute la ligne théorique
  theme_minimal() + # Applique un thème minimal
  theme(
    axis.title.x = element_text(size = 8, color = "darkblue"),
    axis.title.y = element_text(size = 8, color = "darkblue"),
    plot.title = element_text(size = 14, face = "bold", color = "black"),       
    plot.subtitle = element_text(size = 10, color = "black"),                  
    plot.caption = element_text(size = 4, face = "italic", color = "darkgrey") 
  ) 

(p1 / p2) | p3 + 
  plot_layout(guides = "collect")

```

Nous ne constatons pas de valeur aberrante sur le Cleveland plot ou sur l'histogramme.

Nous remarquons grâce au Quantile plot un écart à la normalité. Ceci suggère qu'il est ici intéressant d'utiliser un modèle linéaire généralisé (glm) et non un modèle linéaire classique (lm).

[**L'élévation**]{.underline} **:**

```{r Elevation, echo=FALSE, message=FALSE, warning=FALSE, paged.print=TRUE}

p4 <- ggplot(dataAnts) +
  aes(x=Elevation, y=factor(1:nrow(dataAnts), levels = 1:nrow(dataAnts))) + # Affichage avec classification
  geom_jitter(width=0, height=0.1) +
  scale_x_continuous(breaks = seq(0, 545, by = 100)) + # Valeurs de l'axe X
  labs(title = "Figure 4 : Variable explicative Elevation",
       subtitle = "A. Cleveland Plot",
       x= "Elevation") +  # Titres
  theme_minimal() +
  theme(axis.text.y = element_blank(),   # Supprimer les étiquettes de l'axe Y
        axis.ticks.y = element_blank(),  # Supprimer les ticks de l'axe Y
        axis.title.y = element_blank(),  # Supprimer le titre de l'axe Y
        axis.title.x = element_text(size = 8, color = "darkblue"),
        plot.title = element_text(size = 14, face = "bold", color = "black"),
        plot.subtitle = element_text(size = 10, color = "black")
        )   

p5 <- ggplot(dataAnts) +
  aes(x = Elevation) +  # Utiliser Elevation sur l'axe X
  geom_histogram(binwidth = 75, fill = "blue", color = "black", alpha = 0.7) +  # Histogramme
  scale_x_continuous(breaks = seq(0, 545, by = 100)) +  # Valeurs de l'axe X
  scale_y_continuous(breaks = seq(0, 10, by = 2)) +  # Valeurs de l'axe X
  labs(subtitle = "B. Histogramme",
       x = "Elevation") +  # Titres
  theme_minimal() +  # Appliquer un thème minimal
  theme(
    axis.title.x = element_text(size = 8, color = "darkblue"), 
    axis.title.y = element_text(size = 8, color = "darkblue"),
    plot.subtitle = element_text(size = 10, color = "black")
  ) 

p6 <- ggplot(dataAnts, aes(sample = Elevation)) +
  labs(subtitle = "C. Quantile-Quantile Plot",
       caption = "D'après le jeu de données dataAnts",
       x = "Théorique Quantiles",
       y = "Quantiles Observés") +
  stat_qq() +  # Ajoute les points du Q-Q plot
  stat_qq_line(col = "red", size = 1) +  # Ajoute la ligne théorique
  theme_minimal() + # Applique un thème minimal
  theme(
    axis.title.x = element_text(size = 8, color = "darkblue"),
    axis.title.y = element_text(size = 8, color = "darkblue"),
    plot.title = element_text(size = 14, face = "bold", color = "black"),       
    plot.subtitle = element_text(size = 10, color = "black"),                  
    plot.caption = element_text(size = 4, face = "italic", color = "darkgrey") 
  ) 

(p4 / p5) | p6 + 
  plot_layout(guides = "collect")

```

Nous ne constatons pas de valeur aberrante sur le Cleveland plot ou sur l'histogramme.

Nous remarquons grâce au Quantile plot un écart à la normalité. Ceci suggère qu'il est ici intéressant d'utiliser un modèle linéaire généralisé et non un modèle linéaire classique.

[**L'aire**]{.underline} **:**

```{r Area, echo=FALSE, message=FALSE, warning=FALSE, paged.print=TRUE}

p4 <- ggplot(dataAnts) +
  aes(x=Area, y=factor(1:nrow(dataAnts), levels = 1:nrow(dataAnts))) + # Affichage avec classification
  geom_jitter(width=0, height=0.1) +
  scale_x_continuous(breaks = seq(0, 865000, by = 200000)) + # Valeurs de l'axe X
  labs(title = "Figure 5 : Variable explicative Area",
       subtitle = "A. Cleveland Plot",
       x= "Area") +  # Titres
  theme_minimal() +
  theme(axis.text.y = element_blank(),   # Supprimer les étiquettes de l'axe Y
        axis.ticks.y = element_blank(),  # Supprimer les ticks de l'axe Y
        axis.title.y = element_blank(),  # Supprimer le titre de l'axe Y
        axis.title.x = element_text(size = 8, color = "darkblue"),
        plot.title = element_text(size = 14, face = "bold", color = "black"),
        plot.subtitle = element_text(size = 10, color = "black")
        )   

p5 <- ggplot(dataAnts) +
  aes(x = Area) +  # Utiliser Aera sur l'axe X
  geom_histogram(binwidth = 50000, fill = "blue", color = "black", alpha = 0.7) +  # Histogramme
  scale_x_continuous(breaks = seq(0, 865000, by = 200000)) +  # Valeurs de l'axe X
  scale_y_continuous(breaks = seq(0, 20, by = 5)) +  # Valeurs de l'axe X
  labs(subtitle = "B. Histogramme",
       x = "Area") +  # Titres
  theme_minimal() +  # Appliquer un thème minimal
  theme(
    axis.title.x = element_text(size = 8, color = "darkblue"), 
    axis.title.y = element_text(size = 8, color = "darkblue"),
    plot.subtitle = element_text(size = 10, color = "black")
  ) 

p6 <- ggplot(dataAnts, aes(sample = Area)) +
  labs(subtitle = "C. Quantile-Quantile Plot",
       caption = "D'après le jeu de données dataAnts",
       x = "Quantiles Théorique",
       y = "Quantiles Observés") +
  stat_qq() +  # Ajoute les points du Q-Q plot
  stat_qq_line(col = "red", size = 1) +  # Ajoute la ligne théorique
  theme_minimal() + # Applique un thème minimal
  theme(
    axis.title.x = element_text(size = 8, color = "darkblue"),
    axis.title.y = element_text(size = 8, color = "darkblue"),
    plot.title = element_text(size = 14, face = "bold", color = "black"),       
    plot.subtitle = element_text(size = 10, color = "black"),                  
    plot.caption = element_text(size = 4, face = "italic", color = "darkgrey") 
  ) 

(p4 / p5) | p6 + 
  plot_layout(guides = "collect")

```

Nous constatons des valeurs aberrantes sur le Cleveland plot (Fig. 5A) et sur l'histogramme (Fig. 5B) en raison de la présence de tourbières très étendues. Nous pouvons effectuer une transformation de cette variable et envisager une transformation logarithmique.

```{r LogArea, echo=FALSE, message=FALSE, warning=FALSE, paged.print=TRUE}

dataAnts$LogArea<-log(dataAnts$Area)

p4 <- ggplot(dataAnts) +
  aes(x=LogArea, y=factor(1:nrow(dataAnts), levels = 1:nrow(dataAnts))) + # Affichage avec classification
  geom_jitter(width=0, height=0.1) +
  scale_x_continuous(breaks = seq(2, 15, by = 2)) + # Valeurs de l'axe X
  labs(title = "Figure 6 : Variable explicative LogArea",
       subtitle = "A. Cleveland Plot",
       x= "LogArea") +  # Titres
  theme_minimal() +
  theme(axis.text.y = element_blank(),   # Supprimer les étiquettes de l'axe Y
        axis.ticks.y = element_blank(),  # Supprimer les ticks de l'axe Y
        axis.title.y = element_blank(),  # Supprimer le titre de l'axe Y
        axis.title.x = element_text(size = 8, color = "darkblue"),
        plot.title = element_text(size = 14, face = "bold", color = "black"),
        plot.subtitle = element_text(size = 10, color = "black")
        )   

p5 <- ggplot(dataAnts) +
  aes(x = LogArea) +  # Utiliser Aera sur l'axe X
  geom_histogram(binwidth = 2, fill = "blue", color = "black", alpha = 0.7) +  # Histogramme
  scale_x_continuous(breaks = seq(2, 15, by = 2)) +  # Valeurs de l'axe X
  scale_y_continuous(breaks = seq(0, 30, by = 5)) +  # Valeurs de l'axe X
  labs(subtitle = "B. Histogramme",
       x = "LogArea") +  # Titres
  theme_minimal() +  # Appliquer un thème minimal
  theme(
    axis.title.x = element_text(size = 8, color = "darkblue"), 
    axis.title.y = element_text(size = 8, color = "darkblue"),
    plot.subtitle = element_text(size = 10, color = "black")
  ) 

p6 <- ggplot(dataAnts, aes(sample = LogArea)) +
  labs(subtitle = "C. Quantile-Quantile Plot",
       caption = "D'après le jeu de données dataAnts",
       x = "Quantiles Théorique",
       y = "Quantiles Observés") +
  stat_qq() +  # Ajoute les points du Q-Q plot
  stat_qq_line(col = "red", size = 1) +  # Ajoute la ligne théorique
  theme_minimal() + # Applique un thème minimal
  theme(
    axis.title.x = element_text(size = 8, color = "darkblue"),
    axis.title.y = element_text(size = 8, color = "darkblue"),
    plot.title = element_text(size = 14, face = "bold", color = "black"),       
    plot.subtitle = element_text(size = 10, color = "black"),                  
    plot.caption = element_text(size = 4, face = "italic", color = "darkgrey") 
  ) 

(p4 / p5) | p6 + 
  plot_layout(guides = "collect")

```

Nous ne constatons pas de valeur aberrante sur le Cleveland plot ou sur l'histogramme.

[**Conclusion**]{.underline} **:**

L'analyse de valeurs aberrantes et de la distribution des variables suggère qu'après modification les trois variables quantitatives (covariables) *Latitude*, *Elevation* et *LogArea* peuvent être utilisées dans la construction du modèle.


**c.** $X$ qualitative : analyser le nombre de modalités et le nombre d'individus par modalité

Quand le jeu de données comporte des variables qualitatives (facteurs), il est nécessaire d'analyser l'équilibre des modalités: si il y a plus d'individus qui prennent une modalité par rapport à une autre, leur poids dans le modèle peuvent mener à interprétation finale biaisée.

```{r equilibre des modalites, echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}

summary(dataAnts$Location)

```

Ici les données sont bien réparties entre les deux modalités (Bog = tourbières et Forest = forêts).

[**Conclusion**]{.underline} **:**

L'analyse de valeurs aberrantes et de la distribution de la variable qualitative suggèrent qu'elle peut être utilisée dans la construction du modèle.

-      **2.Analysis of interactions between variables Y and Xs**
       
-        **a.Analyze the potential relationship between Y and Xs**

In order to test the possible relationship between the response variable $Y$ and the explanatory variables $Xs$, it will be essential to carry out statistical modelling, as this is the only way to test the significance of the relationships. However, we can start by performing graphical analyses to better visualize the data and get an idea.

```{r relation Y and XS , echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}

par(mfrow=c(2,2))

# For the latitude variable
plot(dataAnts$Nsp~dataAnts$Latitude,pch=16,col='deepskyblue4',xlab='Latitude',ylab='Ant species richness', main = "Figure 7")

# For the altitude variable
plot(dataAnts$Nsp~dataAnts$Elevation,pch=16,col='deepskyblue4',xlab='Altitude',ylab='Ant species richness', main = "Figure 8")

# For the variable peatland area
plot(dataAnts$Nsp~dataAnts$LogArea,pch=16,col='deepskyblue4',xlab='Peatland area',ylab='Ant species richness', main = "Figure 9")

# For the location variable
boxplot(dataAnts$Nsp~dataAnts$Location, varwidth = TRUE, ylab = "Ant species richness", xlab = "Location", col=c('saddlebrown','palegreen4'), main = "Figure 10")

```

Relationship between $X$ and latitude (Fig. 7): We can see that the higher the latitude, the lower the species richness. We can say that the further north we are, the fewer ant species there are.

Relationship between $X$ and altitude (Fig. 8): In this graph, we can again see that the higher the altitude, the lower the species richness.

Relationship between $X$ and bog area (Fig. 9): Here, there appears to be no relationship between ant species richness and bog area.

Relationship between $X$ and location (Fig. 10): Here, there appears to be a relationship between location and ant species richness, with higher species richness in forests than in bogs.

[**Conclusion**]{.underline} **:**

It seems that the further north and the higher the relief, the lower the ant species richness. What's more, if we're in a forest, the species richness will be greater than in bogs.

-   **b.Analysis of Potential Interactions Among the X Variables**

We aim to study interactions between the factor and the three covariates. Thus, we exclude any interaction among the quantitative explanatory variables. We use a graphical approach to estimate the potential interactive effect among these different variables.

```{r interactions Xs, echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}

par(mfrow=c(1,3))

# Interactions between Latitude & Location
plot(dataAnts$Nsp~dataAnts$Latitude,type='n',ylab = "Ant specific richness",xlab="Latitude", main="Figure 11")
points(dataAnts$Nsp[dataAnts$Location=="Bog"]~dataAnts$Latitude[dataAnts$Location=="Bog"],pch=16,cex=2,col='saddlebrown')
points(dataAnts$Nsp[dataAnts$Location=="Forest"]~dataAnts$Latitude[dataAnts$Location=="Forest"],pch=17,cex=2,col='palegreen4')

# Interactions between Altitude & Localisation
plot(dataAnts$Nsp~dataAnts$Elevation,type='n',ylab = "Ant specific richness",xlab="Elevation", main="Figure 12")
points(dataAnts$Nsp[dataAnts$Location=="Bog"]~dataAnts$Elevation[dataAnts$Location=="Bog"],pch=16,cex=2,col='saddlebrown')
points(dataAnts$Nsp[dataAnts$Location=="Forest"]~dataAnts$Elevation[dataAnts$Location=="Forest"],pch=17,cex=2,col='palegreen4')

# Interactions between Superficie de la tourbière & Localisation
plot(dataAnts$Nsp~dataAnts$LogArea,type='n',ylab = "Ant specific richness",xlab="Bog area", main="Figure 13")
points(dataAnts$Nsp[dataAnts$Location=="Bog"]~dataAnts$LogArea[dataAnts$Location=="Bog"],pch=16,cex=2,col='saddlebrown')
points(dataAnts$Nsp[dataAnts$Location=="Forest"]~dataAnts$LogArea[dataAnts$Location=="Forest"],pch=17,cex=2,col='palegreen4')

```

Generally, we can assume an interaction between covariates if the slopes formed by the two point clouds are different. Here, the clouds do not clearly reveal any similarity or difference in trend for each graph. It is therefore difficult to draw conclusions about these interactions.

-   **c.Checking for Potential Collinearity Among the X Variables**

We aim to avoid collinearity among the explanatory variables in our modeling to prevent multiple variables from providing the same information. To do this, we must examine the statistical relationships that may exist among the variables by calculating the correlation between quantitative $X$ variables, and analyzing the influence of the qualitative $X$ on the quantitative $X$ variables using plot graphics.

```{r colinearity1, echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}

# Checking for collinearity between independent continuous variables
# Creating a scatterplot for each pair of covariates
newdata<-cbind(dataAnts$Latitude,dataAnts$Elevation,dataAnts$LogArea)
colnames(newdata)<-c('Latitude','Elevation','Log Bog area')
newdata<-data.frame(newdata)
plot(newdata,pch=16,col='deepskyblue4',main="Figure 14")

# Calculating correlation for each pair of covariates
M<-cor(newdata)
corrplot.mixed(M,upper="square",lower.col="black", tl.col="black",cl.cex = 0.8,tl.cex = 0.7,number.cex =0.8)
mtext("Figure 15", side = 2, line = -6, cex = 1, las = 1)

par(mfrow=c(1,3))

# Checking for collinearity between categories and continuous variables
# Latitude and Location
boxplot(dataAnts$Latitude~dataAnts$Location, varwidth = TRUE, ylab = "Latitude", xlab = "Location",col=c('saddlebrown','palegreen4'), main = "Figure 16")

# Altitude and Location
boxplot(dataAnts$Elevation~dataAnts$Location, varwidth = TRUE, ylab = "Elevation", xlab = "Location",col=c('saddlebrown','palegreen4'), main = "Figure 17")

# Bog Area and Location
boxplot(dataAnts$LogArea~dataAnts$Location, varwidth = TRUE, ylab = "Bog area", xlab = "Location",col=c('saddlebrown','palegreen4'), main = "Figure 18")

```

In the first graph (Fig. 14), we can see that the point clouds do not appear to show any trend toward collinearity. This impression is confirmed by the calculated values in the following figure (Fig. 15), where the values are well below 0.7, which is commonly used as a threshold for collinearity (above this, variables are considered correlated). The almost complete overlap of the boxplots (Figs. 16, 17, 18) further supports the non-collinearity of these factors.

[**Conclusion**]{.underline} **:**

There does not appear to be any colinearity among the variables, so we can retain them all.

# ANALYSE STATISTIQUE


    1.  **Construction du modèle**


Nous cherchons à produire le meilleur modèle pour prédire le nombre d'espèces. La variable réponse est un comptage (variable quantitative discrète) de petit effectifs (\<30), la loi appropriée est donc la loi de **Poisson**. Nous allons donc réaliser la modélisation avec un modèle linéaire généralisé de Poisson.

Pour arriver au modèle, dit "candidat", à tester, nous allons partir du modèle dit "complet", c'est-à-dire avec toutes les variables explicatives et leurs interactions, et utiliser une méthode pas à pas arrière ou **Backward selection** pour sélectionner les variables et les interactions à conserver.

Commençons avec le modèle complet:

```{r, echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}

# Formulation du modèle avec la fonction glm() et family=poisson(link="log")
mod1<-glm(Nsp~ Location
        + Latitude
        + Elevation
        + LogArea
        + Location:Latitude
        + Location:Elevation
        + Location:LogArea
        ,data=dataAnts
        ,family=poisson(link="log"))

```

A partir de ce modèle complet nous allons éliminer pas à pas les interactions non significatives.

```{r colinéarité2, echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}

# Etape 1
drop1(mod1,test="Chi")

```

Les trois interactions ne sont pas significatives. Nous enlèvons l'interaction 'Location:Latitude' car c'est la 'moins significative'. Nous pouvons également remarquer que la fonction drop1() nous informe que c'est en enlevant cette interaction que nous obtenons le meilleur AIC.

Nous obtenons donc le modèle suivant:

```{r, echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}

# Etape 2
mod1<-glm(Nsp~ Location
        + Latitude
        + Elevation
        + LogArea
        + Location:Elevation
        + Location:LogArea
        ,data=dataAnts
        ,family=poisson(link="log"))

drop1(mod1,test="Chi")

```

De la même façon, nous enlèvons l'interaction 'Location:LogArea'.

```{r, echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}

# Etape 3
mod1<-glm(Nsp~ Location
        + Latitude
        + Elevation
        + LogArea
        + Location:Elevation
        ,data=dataAnts
        ,family=poisson(link="log"))

drop1(mod1,test="Chi")

```

Nous enlèvons également l'interaction 'Location:Elevation'. Nous regardons maintenant la significativité des variables explicatives.

```{r, echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}

# Etape 4
mod1<-glm(Nsp~ Location
        + Latitude
        + Elevation
        + LogArea
        ,data=dataAnts
        ,family=poisson(link="log"))

drop1(mod1,test="Chi")

```

La variable 'LogArea' n'a pas un effet siginificatif, nous l'éliminons du modèle.

```{r, echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}

# Etape 5
mod1<-glm(Nsp~ Location
        + Latitude
        + Elevation
        ,data=dataAnts
        ,family=poisson(link="log"))

drop1(mod1,test="Chi")

```

Les variables explicatives restantes sont significatives, nous conservons ainsi ce modèle comme modèle candidat.

Nous allons ensuite identifier le modèle candidat avec une autre approche que la significativité des p-values : l'AIC avec le mode stepwise.

```{r model AIC, echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}

# Nous construisons le modèle complet
modelcomplet<- glm(Nsp~ Location
        + Latitude
        + Elevation
        + LogArea
        + Location:Latitude
        + Location:Elevation
        + Location:LogArea
        ,data=dataAnts
        ,family=poisson(link="log"))

# Sélection du modèle sur la base AIC
modfinal<-stepAIC(modelcomplet,direction="both")

# Analyse du modèle final
drop1(modfinal,test="Chi")
summary(modfinal)

```

Remarque : il est recommandé d'utiliser l'AIC avec plus de 40 individus par paramètre, sinon il existe l'AICc (@YannickOutreman).

[**Conclusion**]{.underline} **:**

Les deux méthodes arrivent au même modèle candidat : *Nsp\~Latitude+Elevation+LogArea*.

-   <div>

    2.  **Analyse des coefficients**

    </div>

L'étape suivante est l'analyse des coefficients. Nous allons donc regarder l'effet précis des variables explicatives retenues (Latitude, Elevation, LogArea) sur la richesse spécifique des fourmis.

```{r, echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}

# Coefficients du modèle
summary(mod1)
#                 Estimate Std. Error z value Pr(>|z|)    
#(Intercept)    11.9368121  2.6214970   4.553 5.28e-06 ***
#LocationForest  0.6354389  0.1195664   5.315 1.07e-07 ***
#Latitude       -0.2357930  0.0616638  -3.824 0.000131 ***
#Elevation      -0.0011411  0.0003749  -3.044 0.002337 ** 

#Null deviance: 102.76  on 43  degrees of freedom
#Residual deviance:  40.69  on 40  degrees of freedom

```

Le summary() du modèle nous permet de voir les coefficients correspondant à chaque variable ainsi que l'intercept (basé sur la moyenne de la population). Pour un facteur (variable qualitative) le coefficient correspond à la comparaison entre la modalité donnée et la modalité dite "de référence" dont le coefficient est égal à 0. Les scores de déviance serviront dans la partie suivante. Grâce à ce tableau nous pouvons noter les différents coefficients :

**Location factor**\
- $Location_{Bog}$ = 0 (modalité de référence du facteur Location)\
- $Location_{Forest}$ = $+0.63^{**}$\
**Latitude covariate**\
- $\beta_{Latitude}$ = $-0.23^{***}$\
**Elevation covariate**\
- $\beta_{Elevation}$ = $-0.001^{***}$

Le modèle candidat avec les coefficients s'écrit (équation 5) : $$ log(Species\:Richness) = 11.93^{***} + (Location_{Bog} = 0 ;\:Location_{Forest} = +0.63^{***})\:- 0.23^{***}.Latitude\: -0.001^{***}. Elevation  $$ {#eq-5}

Rappel : pour modéliser une variable quantitative discrète avec un modèle linéaire généralisé nous passons par une fonction de lien, ici avec la loi de Poisson nous utilisons la fonction log().

-   <div>

    3.  **Pouvoir explicatif du modèle**

    </div>

Afin de connaître la qualité de notre modèle, nous allons calculer la distance entre le modèle candidat et le modèle nul. Le modèle nul est un modèle qui résume les données avec un unique paramètre : la moyenne de $Y$. Ce modèle n'explique pas les données.

Un moyen de connaître la qualité du modèle est de calculer un *pseudo R²*. Pour cela, nous déterminons la distance entre la déviance du modèle nul et la déviance résiduelle du modèle candidat (équation 6).

$$Pseudo\:R^2=100\:.\:\frac{Déviance\:nulle- Déviance\:résiduelle}{Déviance\:nulle}$$ {#eq-6}

Nous pouvons calculer le *pseudo R²* de trois manières différentes : avec la méthode de McFadden, celle de Cox et Snell ou celle de Nagelkerke. Les deux dernières méthodes nécessitent le package 'rcompagnon'.

Dans notre exemple, comme vu dans la partie précédente, le modèle nul a une déviance de 102,76 et la déviance résiduelle est de 40,69.

Nous allons donc calculer la qualité du modèle avec les commandes suivantes :

```{r deviance, echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}

# Estimate of deviance explained
(mod1$null.deviance-mod1$deviance)/mod1$null.deviance

# Some others estimates of deviance explained - package 'rcompanion'
library(rcompanion)
nagelkerke(mod1)

```

Ici, l'estimation de la déviance expliquée est de 60% ou 75%, en fonction de la méthode utilisée.

```{r prediction, echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}

plot(fitted(mod1)~dataAnts$Nsp,col='dodgerblue3',pch=16,ylab="Valeurs estimées par le modèle",xlab="Valeurs observées", main="figure 19")
abline(0,1)

```

-   <div>

    4.  **Validation du modèle**

    </div>

L'*indépendance des résidus* est la seule chose qui limite le modèle linéaire généralisé. L'analyse des résidus permet donc d'identifier d'éventuelles tendances et de vérifier la présence d'unités statistiques influentes.

Dans un premier temps, vérifier la présence de **surdispersion** est indispensable lorsque l'on fait un modèle linéaire généralisé suivant une loi de Poisson. Pour tous les modèles linéaires généralisés utilisant des données de comptage, il faut vérifier l'absence de surdispersion.

Pour cela, nous allons calculer un paramètre appelé 'scale parameter'. Si sa valeur est nettement supérieure à 1 (à partir de 1.6 ou 1.7), il y a surdispersion. Le modèle n'est donc pas valable.

**a.Vérification de la surdispersion**

Quand l'indice de dispersion est supérieur à 1.5, il existe différentes explications.

Cela peut être dû à :

-   Des valeurs abérrantes sont présentes dans le jeu de données. Il faut donc les éliminer.

-   Des covariables sont manquantes. Il faut les ajouter au modèle.

-   Des interactions sont manquantes. Il faut les ajouter au modèle.

-   Une inflation des zéros se produit. Il faut utiliser une regression Binomiale négative à inflation des zéros. (@Long_J.S._1997)

-   Une dépendance entre les données et un facteur ou une variable est présente. Il faut utiliser un modèle linéaire général mixte. (@Johnson_et_al._2015)

-   Relation entre les données et les variables ou facteurs n'est pas linéaire. Il faut utiliser un modèle additif généralisé. (@Irz_et_Levi-Valensin_2016)

-   La fonction de lien est mauvaise. Il faut changer la fonction de lien.

-   La variation de $Y$ est grande. Il faut utiliser un modèle linéaire généralisé suivant une loi binomiale négative. (@Chahine_1965)

Dans notre exemple, nous allons calculer le 'scale parameter'. Pour cela nous allons utiliser la commande :

```{r surdispersion, echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}

# Scale parameter calculation
E1 <- resid(mod1, type = "pearson") # (Y - mu) / sqrt(mu)
N  <- nrow(dataAnts)
p  <- length(coef(mod1))
sum(E1^2) / (N - p)

```

[**Conclusion**]{.underline} **:**

Ici, la dispersion est de 1.02. Il n'y a pas de surdispersion.

**b.Analyses des résidus**

Si les hypothèses de normalité et d'homogénéité des résidus ne sont pas attendues dans les modèles linéaires généralisés, nous pouvons cependant les analyser graphiquement. En traçant les résidus en fonction des $Xs$ significatifs du modèle, nous devons valider l'absence de tendance dans leur distribution. Si nous constatons une tendance, la modélisation peut être problématique : cela peut être dû à un manque d'ajustement, à une dépendance dans les données ou à des observations influentes. Rappelons que dans les modèles linéaires généralisés, nous utilisons **les résidus de Pearson** car ils incluent l'hétérogénéité de la variance et sont faciles à calculer et à comprendre.

```{r, echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}

resid<-residuals(mod1, type="pearson")

par(mfrow=c(3,2))
# Histogram
hist(resid,col='dodgerblue3',xlab="residuals",main="figure 20")
# Quantile-Quantile plot
qqnorm(resid,pch=16,col='dodgerblue3',xlab='', main="figure 21")
qqline(resid,col='red',lwd=2)

# Residuals vs fitted
plot(resid~fitted(mod1)
      , col='dodgerblue3'
      , pch=16, main="figure 22")
abline(h = 0)

# Residuals against Location factor
boxplot(resid~ dataAnts$Location, 
         varwidth = TRUE,
         ylab = "Residuals",
         xlab = "Location",
         main = "figure 23")

# Residuals against Latitude
plot(resid~ dataAnts$Latitude, 
         pch=16,
         col="dodgerblue3",
         ylab = "Residuals",
         xlab = "Latitude",
         main = "figure 24")
abline(h = 0)

# Residuals against Elevation
plot(resid~ dataAnts$Elevation, 
         pch=16,
         col="dodgerblue3",
         ylab = "Residuals",
         xlab = "Elevation",
         main = "figure 25")
abline(h = 0)

```

L'histogramme des résidus (Fig. 20) et le Q-Q plot (Fig. 21) montrent des résidus relativement normaux. Les quatres graphiques suivant (Fig. 22, 23, 24, 25) montrent la distribution les résidus selon les estimations du modèle et les trois variables explicatives. Nous pouvons voir que les résidus restent homogènes peu importe les valeurs des variables explicatives.

[**Conclusion**]{.underline} **:**

Nous pouvons conclure qu'il n'y a pas de tendance dans les résidus.

**c.Vérification des individus influents**

```{r contri, echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}

par(mfrow = c(1, 1))
plot(cooks.distance(mod1), type = "h", ylim = c(0, 1), main="figure 26")
abline(h = 1, col = 2,lwd = 3)

```

[**Conclusion**]{.underline} **:**

Ici (Fig. 26), il n'y a pas d'individus trop influent.

**d.Analyses des résidus avec DHARMa**

Pour une représentation graphique différente, nous pouvons utiliser le package 'DHARMa'. Ce package permet d'analyser les résidus et plus particulièrement leur surdispersion et les valeurs aberrantes. Ce package met en couleur rouge les résidus ne respectant pas les valeurs attendues.

```{r Dharma, echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}

# TestDispersion(mod1)
library(DHARMa)
simulationOutput <- simulateResiduals(fittedModel = mod1, plot = F)
residuals(simulationOutput)
plot(simulationOutput)
residuals(simulationOutput, quantileFunction = qnorm, outlierValues = c(-7,7))

# Représentation des résidus selon la covariable
plotResiduals(simulationOutput, form = dataAnts$Latitude)

```

[**Conclusion**]{.underline} **:**

Dans notre exemple, les résidus ne sont pas surdispersés et ne montrent pas de valeurs aberrantes.

# CONCLUSION

The selected model is as follows (equation 5):

$$ log(Species\:Richness) = 11.93^{***} + (Location_{Bog} = 0 ;\:Location_{Forest} = +0.63^{***})\:- 0.23^{***}.Latitude\: -0.001^{***}. Elevation  $$

Its dispersion index is estimated at 1.02, indicating the absence of overdispersion, which validates our model.

Furthermore, we detect no influential data points that could bias the model.

The estimated pseudo R² of 0.60 suggests that this model effectively explains our data (the null model deviance is 102.76, while the residual deviance is 40.69).

We conclude that ant species richness in the New England bogs depends on:

-   **Location**: Species richness is higher in forests than in bogs, linked to vegetation cover and available light;

-   **Latitude**: Species richness decreases with increasing latitude;

-   **Elevation**: Although its effect is smaller, elevation also influences species richness.

In both forests and bogs, latitude is the most significant predictor of species density.

However, further research on a larger scale is necessary to generalize these findings, as this study covers only a limited geographical area with two habitat types.
