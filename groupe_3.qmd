---
title: "Groupe 3"
autor: "Gabriel-Marie Arnault, Giuliana Brambilla, Apolline Byrdy, Anaelle Hulbert, Manon Le Bihan, Kilian Prevost, Chloé Van der Swaelmen et Léa Vuille"
source : "Yannick Outreman, selon les données de Gotelli & Ellison (2002) - Biogeography at a regional scale : determinants of antspecies density in New England bogs and forests. Ecology, 83 : 1604-1609."
editor: visual
---

# INTRODUCTION

-   **1.Présentation de l'article**

Nos données sont issues de l'article "Biogeography at a regional scale : determinants of ant species density in New England bogs and forests" de Gotelli & Ellison publié dans *Ecology* en 2002 (83(6), 2002, pp. 1604--1609) (https://doi.org/10.1890/0012-9658(2002)083\[1604:BAARSD\]2.0.CO;2)

-   **2.Contexte Biologique**

Gotelli et Ellison sont deux écologues, le premier étant spécialisé sur des questions d'organisation des communautés animales et végétales, le second sur la désintégration et le réassemblage des écosystèmes suite à des perturbations naturelles et anthropiques. Ils se sont ici intéressés à l'influence du gradient latitudinal sur la richesse spécifique chez les fourmis de l'Etat de la Nouvelle-Angleterre au Nord-Est des Etats-Unis. Leur étude a été réalisée sur 22 sites où, pour chacun, 25 pièges ont été placés dans une tourbière ombrotrophe et 25 dans la forêt environnante. Les espèces de fourmis contenues dans les pièges sont par la suite identifiées en laboratoire et, pour un site et un habitat donnés, ils ont calculé la richesse spécifique (nombre total d'espèces présentes).

-   **3.Données et variables**

Les variables utilisées sont :\
- *Site* : le nom du site échantillonné\
- *Latitude* : la latitude du site (quantitative)\
- *Altitude* : l'altitude du site (quantitative)\
- *Area* : la superficie de la tourbière du site (quantitative)\
- *Location* : la localisation des pièges ("Bog" ou "Forest") (qualitative)\
- *Nsp* : la richesse spécifique des fourmis, correspondant à la variable réponse

-   **4.Question de recherche**

Parmi les trois questions que se posent Gotelli et Ellison dans cette étude, nous nous intéressons ici aux deux premières :

1\) La densité des espèces est-elle en corrélation avec la latitude sur une étendue latitudinale aussi étroite ?

2\) Le gradient latitudinal persiste-t-il après un contrôle statistique des différences entre les sites dans la composition de la végétation ?

En d'autres termes : quelles sont les variables quantitatives (latitude, altitude, aire) et qualitatives (localisation) qui déterminent la richesse spécifique des fourmis dans les tourbières du New-England ?

-   **5.Importation du jeu de données**

Dès lors, on peut importer le jeu de données situé dans fichier "BogAnts.txt"

```{r Importation du jeu de donnees, echo=TRUE, message=FALSE, include=TRUE}

# Configuration de la session :
rm(list=ls()) # Nettoyage de l'environnement de travail

# Importation
dataAnts <- read.table("BogAnts.txt", dec=".", header = TRUE)

# Passer la localisation en facteur 
dataAnts$Location<-as.factor(dataAnts$Location)

# Vérification du type des variables 
str(dataAnts)

```

[**Conclusion**]{.underline} **:**

Les variables qualitatives ou quantitatives sont bien considérées comme telles.

-   **6.Quel modèle et pourquoi ?**

Pour modéliser la relation entre une variable réponse quantitative $Y$ et des variables explicatives $X_{1}$,$X_{2}$...$X_{p}$, quantitatives et/ou qualitatives nous utilisons des modèles linéaires généraux, ou *glm*, (régression linéaire, ANCOVA, ANOVA). Pour pouvoir utiliser ces *glm*, trois conditions d'application sont nécessaires : *l'homogénéité des variances*, *l'indépendance des résidus* et *la normalité des résidus*. Cependant, les réponses $Y$ analysées en écologie, peuvent parfois être discrètes et s'écarter de l'hypothèse de normalité.

Pour les variables réponses discrètes (comptages, données binaires), la variance de $Y$ varie généralement avec la moyenne de $Y$. Cette relation semble alors indiquer que la variance ne peut être homogène dans un jeu de données, du fait de sa dépendance à la moyenne. Or, les modèles linéaires généraux ont pour hypothèse d'application une variance constante (c'est-à-dire une homogénéité de la variance). Par ailleurs, lorsque l'on applique un modèle linéaire général à des données de comptage, les valeurs prédites obtenues peuvent être négatives (ce qui est impossible au regard de la nature de $Y$) et les résidus non distribués normalement. Donc, l'utilisation d'un modèle linéaire général (*glm*) sur des réponses discrètes génère très généralement des écarts par rapport aux conditions d'application. En conclusion, il devient necessaire d'utiliser une méthode plus adaptée à l'analyse de variables réponses discrètes telle que les modèles linéaires généralisés (*glim*).

**Voici les étapes à suivre pour établir un modèle linéaire généralisé :**\
1. Formuler l'hypothèse de la loi de distribution de la variable réponse $Y_{i}$\
2. Explorer les données (valeurs aberrantes, distribution des valeurs,)\
3. Analyser les interactions entre les variables Y et Xs (analyse des relations entre X et Y, entre Xs, et recherche de colinéarité entre Xs)\
4. Procéder aux analyses statistiques (construction du modèle, analyse des coefficients, pouvoir explicatif du modèle)\
5. Conclure quant aux résultats

Une modélisation linéaire généralisée s'écrit comme suit (équation 1) : $$g(\mu_{y})= \alpha+ \beta_{1}.X_{i1}+ \beta_{2}.X_{i2}+\beta_{3}.X_{i3}+...\beta_{p}.X_{ip} = \eta $$ {#eq-1} Le prédicteur linéaire, $\eta$, est issu du modèle linéaire comme la somme des $p$ termes associés à chacun des paramètres $\beta_{p}$. Sa valeur est obtenue en transformant la valeur de $Y$ par la fonction de lien, et la valeur prédite de $Y$ est obtenue en appliquant à $\eta$ la fonction inverse de la fonction de lien.

Pour illustrer ce propos nous nous intéressons à des comptages. Un comptage est une variable aléatoire discrète et positive. Il est possible que la distribution de ses valeurs suive une loi de Poisson, Binomiale Négative voire une loi Normale lorsque les valeurs sont très importantes.

Notre exemple illustre une distribution suivant la loi de Poisson.

La loi de Poisson est utile pour décrire des évènements discrets rares et indépendants. Ainsi, une variable $Y$ qui suit une loi de Poisson de paramètre $\lambda$ s'écrit comme suit (équation 2) : $$Pr(Y=y)=\frac{e^{-\lambda}.\lambda^y}{y!}$$ {#eq-2} où $y$ représente le nombre observé d'occurences (i.e., $y$=0, 1, 2...} et $\lambda$ la moyenne et la variance de la distribution de Poisson (i.e. dans la loi de Poisson : E($y$)= Var($y$)=$\lambda$).

Dans un *glim* type Poisson, la fonction de lien est la fonction **log**. Ainsi, le modèle s'écrit (équation 3): $$log(\mu_{y})= \alpha+ \beta_{1}.X{i1}+ \beta_{2}.X{i2}+\beta_{3}.X{i3}+...\beta_{p}.X{ip} = \eta $$ {#eq-3} Les valeurs prédites de $Y$ sont obtenues en appliquant sur $\eta$ la fonction inverse de la fonction de lien, ici la fonction **log** (équation 4) : $$\mu_{y}= e^{\alpha+ \beta_{1}.X{i1}+ \beta_{2}.X{i2}+\beta_{3}.X{i3}+...\beta_{p}.X{ip}} = e^{\eta} $$ {#eq-4}

Les modèles linéaires généralisés de type Poisson ont une structure d'erreur qui suivent une loi de Poisson. Cette structure permet, entre autres, de définir avec précision la relation entre la moyenne et la variance. Cette relation est exploitée par la méthode du maximum de vraisemblance pour estimer les coefficients et les erreurs standard des paramètres du modèle GLM.

-   **7.Configuration de l'environnement et packages**

Pour analyser nos données nous configurons la session en nettoyant l'espace de travail. Nous aurons aussi besoin de charger les packages suivants : *knitr*, *ggplot2*, *tinytext*, *corrplot*, *plot3D*, *DHARMa*, *rcompanion*, *lattice*, *patchwork*, *MASS* et *rcompanion*

```{r init, include=FALSE, message=FALSE, warning=FALSE}

# Chargement des packages R :
library(knitr)
library(ggplot2) # Package pour de meilleurs visuels graphiques
library(tinytex) # Pour la sortie pdf
library(corrplot) # Calcul de matrice de corrélation
library(plot3D) # Pour les graphiques 3D
library(DHARMa) # Diagnostic du modèle
library(rcompanion) # Modèle pseudo R²
library(lattice) # Graphiques multipanneaux (A CHANGER?)
library(patchwork) # Afficher plusieurs ggplot sur une page
library(MASS) # Fonctions et datasets pour les analyses

```

-   **8.Vérification si des données sont manquantes**

```{r datalack, echo=TRUE,include=TRUE}

# Vérification de la présence de valeurs manquantes
colSums(is.na(dataAnts))
summary(dataAnts)

```

Il n'y a aucune valeurs manquantes.

-   **9.Déroulé de l'analyse**

A partir de cela, nous allons dans un premier temps examiner le jeu de données (valeurs abérantes, potentielles relations entre Y et Xs, ...) afin d'avoir une première impression. Par la suite, nous réaliserons notre analyse statistique en constuisant et affinant notre modèle. Enfin, nous analyserons ses coefficients et son pouvoir explicatif afin de le valider ou non.

# EXPLORATION DU JEU DE DONNEES

-   **1.Présentation du jeu de données**

    **a.Valeurs aberrantes dans** $Y$ et distribution des valeurs de $Y$

La première étape de l'exploration du jeu de données consiste à vérifier la présence de potentielles valeurs aberrantes dans la variable réponse $Y$ et à examiner la distribution de ses valeurs.

**Valeurs aberrantes :** dans un jeu de données écologiques, il est probable de rencontrer des individus présentant des valeurs extrêmes pour la variable réponse. Ces individus, en raison de leurs valeurs inhabituelles, peuvent avoir un impact disproportionné sur la construction du modèle. Toutefois, ces valeurs extrêmes peuvent résulter soit de la variabilité naturelle des phénomènes écologiques, soit d'erreurs (par exemple, des erreurs de mesure ou de manipulation). Identifier ces individus avant l'analyse statistique permet de prévenir les erreurs et de justifier, si nécessaire, l'exclusion de certaines valeurs aberrantes pour améliorer la robustesse du modèle.

Les valeurs aberrantes peuvent être identifiées grâce à un **Boxplot** ou à un **Cleveland plot** des valeurs de $Y$.

```{r valeurs aberrantes de Y, echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}

# Boxplot 
bp <- ggplot(data = dataAnts, aes(y = Nsp))+
  geom_boxplot()+
  labs(subtitle = "A. Boxplot",
       y = "richesse spécifique")+
  theme(
    panel.background = element_rect(fill="white"),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    
    axis.title.x = element_text(size = 8, color = "darkblue"),
    axis.title.y = element_text(size = 8, color = "darkblue"),
    plot.title = element_text(size = 14, face = "bold", color = "black"),       
    plot.subtitle = element_text(size = 10, color = "black"),                  
    plot.caption = element_text(size = 4, face = "italic", color = "darkgrey")
  )

# Cleveland plot 
cp <- ggplot(data = dataAnts, aes(x = Nsp, y = 1:nrow(dataAnts)))+
  geom_jitter(width = 0.1, size = 3, color = "black")+
  labs(subtitle = "B. Cleveland plot",
       x = "richesse spécifique")+
  theme_minimal()+
  theme(
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    axis.title.y = element_blank(),
    
    axis.title.x = element_text(size = 8, color = "darkblue"),
    plot.title = element_text(size = 14, face = "bold", color = "black"),       
    plot.subtitle = element_text(size = 10, color = "black"),                  
    plot.caption = element_text(size = 4, face = "italic", color = "darkgrey")
  )

## Affichage 
(bp + cp) + 
  plot_layout(widths = c(1, 2)) + 
  plot_annotation(title = "Figure 1 : Analyse des valeurs aberrantes de Y")

```

D'après la figure 1, $Y$ ne présente pas de valeurs extrêmes ou aberrantes.

**Distribution des valeurs :** analyser la distribution des valeurs de $Y$ permet d'identifier *a priori* la loi de probabilité qui expliquera au mieux les processus écologiques en cours. Cela est d'autant plus intéressant si les données suggèrent une relation non linéaire ou asymétrique.

La distribution des valeurs de $Y$ peut être visualisée grâce à un **histogramme** ou à un **Normal Q-Q plot**.

```{r distribution de Y, echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}

# Histogramme 
hg <- ggplot(data = dataAnts, aes(x = Nsp))+
  geom_histogram(binwidth = 2, fill = "blue", color = "black")+
  theme_minimal()+
  labs(subtitle = "A. Histogramme",
       x = "richesse spécifique",
       y = "fréquence")+
  theme(
    axis.title.x = element_text(size = 8, color = "darkblue"),
    axis.title.y = element_text(size = 8, color = "darkblue"),
    plot.title = element_text(size = 14, face = "bold", color = "black"),       
    plot.subtitle = element_text(size = 10, color = "black"),                  
    plot.caption = element_text(size = 4, face = "italic", color = "darkgrey")
    )

qqp <- ggplot(data = dataAnts, aes(sample = Nsp))+
  geom_qq_line(color = "red", size = 1.2)+
  geom_qq()+
  theme_minimal()+
  labs(subtitle = "B. Q-Q plot",
       x = "quantiles théoriques",
       y = "quantiles observés")+
  theme(
    axis.title.x = element_text(size = 8, color = "darkblue"),
    axis.title.y = element_text(size = 8, color = "darkblue"),
    plot.title = element_text(size = 14, face = "bold", color = "black"),       
    plot.subtitle = element_text(size = 10, color = "black"),                  
    plot.caption = element_text(size = 4, face = "italic", color = "darkgrey")
  )

# Affichage
(hg + qqp) +
  plot_layout(widths = c(1, 2)) + 
  plot_annotation(title = "Figure 2 : Analyse de la distribution de Y")

```

La distribution de $Y$ (exponentielle négative) et la nature du jeu de données (comptage sans aggrégation) suggèrent que $Y$ suit une loi de Poisson. Cette hypothèse est appuyée par l'écart à la normalité des valeurs de $Y$ observé en figure 2B.

[**Conclusion**]{.underline} **:**

Après analyse des valeurs aberrantes et de la distribution de la variable réponse $Y$, il semble pertinent d'utiliser $Y$ tel quel en appliquant une loi de Poisson.

**b.** $X$ quantitative : présence de valeurs abérrantes et distribution de valeurs de $X$

Le jeu de données dispose de trois variables explicatives quantitatives : *Latitude*, *Elevation* et *Area.* De même que pour la variable réponse Y, il est nécessaire d'observer la présence de valeurs aberrantes ainsi que la distribution de chacune des variables réponses.

[**La latitude**]{.underline} **:**

```{r Latitude, echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}

p1 <- ggplot(dataAnts) +
  aes(x=Latitude, y=factor(1:nrow(dataAnts), levels = 1:nrow(dataAnts))) + # Affichage avec classification
  geom_jitter(width=0, height=0.1) +
  scale_x_continuous(breaks = seq(41.5, 45, by = 0.5)) + # Valeurs de l'axe X
  labs(title = "Figure 3 : Variable explicative Latitude",
       subtitle = "A. Cleveland Plot",
       x= "Latitude") +  # Titres
  theme_minimal() +
  theme(axis.text.y = element_blank(),   # Supprimer les étiquettes de l'axe Y
        axis.ticks.y = element_blank(),  # Supprimer les ticks de l'axe Y
        axis.title.y = element_blank(),  # Supprimer le titre de l'axe Y
        axis.title.x = element_text(size = 8, color = "darkblue"),
        plot.title = element_text(size = 14, face = "bold", color = "black"),
        plot.subtitle = element_text(size = 10, color = "black")
        )   

p2 <- ggplot(dataAnts) +
  aes(x = Latitude) +  # Utiliser Latitude sur l'axe X
  geom_histogram(binwidth = 0.25, fill = "blue", color = "black", alpha = 0.7) +  # Histogramme
  scale_x_continuous(breaks = seq(41.5, 45, by = 0.5)) +  # Valeurs de l'axe X
  scale_y_continuous(breaks = seq(0, 10, by = 2)) +  # Valeurs de l'axe X
  labs(subtitle = "B. Histogramme",
       x = "Latitude") +  # Titres
  theme_minimal() +  # Appliquer un thème minimal
  theme(
    axis.title.x = element_text(size = 8, color = "darkblue"), 
    axis.title.y = element_text(size = 8, color = "darkblue"),
    plot.subtitle = element_text(size = 10, color = "black")
  ) 

p3 <- ggplot(dataAnts, aes(sample = Latitude)) +
  labs(subtitle = "C. Quantile-Quantile Plot",
       caption = "D'après le jeu de données dataAnts",
       x = "Quantiles Théorique",
       y = "Quantiles Observés") +
  stat_qq() +  # Ajoute les points du Q-Q plot
  stat_qq_line(col = "red", size = 1) +  # Ajoute la ligne théorique
  theme_minimal() + # Applique un thème minimal
  theme(
    axis.title.x = element_text(size = 8, color = "darkblue"),
    axis.title.y = element_text(size = 8, color = "darkblue"),
    plot.title = element_text(size = 14, face = "bold", color = "black"),       
    plot.subtitle = element_text(size = 10, color = "black"),                  
    plot.caption = element_text(size = 4, face = "italic", color = "darkgrey") 
  ) 

(p1 / p2) | p3 + 
  plot_layout(guides = "collect")

```

On ne constate pas de valeurs aberrantes sur le Cleveland plot (Fig. 3A) ou sur l'histogramme (Fig. 3B).

On remarque grâce au Quantile plot (Fig. 3C) un écart à la normalité. Ceci suggère qu'il est ici intéressant d'utiliser un modèle linéaire généralisé (glm) et non un modèle linéaire classique (lm).

[**L'élévation**]{.underline} **:**

```{r Elevation, echo=FALSE, message=FALSE, warning=FALSE, paged.print=TRUE}

p4 <- ggplot(dataAnts) +
  aes(x=Elevation, y=factor(1:nrow(dataAnts), levels = 1:nrow(dataAnts))) + # Affichage avec classification
  geom_jitter(width=0, height=0.1) +
  scale_x_continuous(breaks = seq(0, 545, by = 100)) + # Valeurs de l'axe X
  labs(title = "Figure 4 : Variable explicative Elevation",
       subtitle = "A. Cleveland Plot",
       x= "Elevation") +  # Titres
  theme_minimal() +
  theme(axis.text.y = element_blank(),   # Supprimer les étiquettes de l'axe Y
        axis.ticks.y = element_blank(),  # Supprimer les ticks de l'axe Y
        axis.title.y = element_blank(),  # Supprimer le titre de l'axe Y
        axis.title.x = element_text(size = 8, color = "darkblue"),
        plot.title = element_text(size = 14, face = "bold", color = "black"),
        plot.subtitle = element_text(size = 10, color = "black")
        )   

p5 <- ggplot(dataAnts) +
  aes(x = Elevation) +  # Utiliser Elevation sur l'axe X
  geom_histogram(binwidth = 75, fill = "blue", color = "black", alpha = 0.7) +  # Histogramme
  scale_x_continuous(breaks = seq(0, 545, by = 100)) +  # Valeurs de l'axe X
  scale_y_continuous(breaks = seq(0, 10, by = 2)) +  # Valeurs de l'axe X
  labs(subtitle = "B. Histogramme",
       x = "Elevation") +  # Titres
  theme_minimal() +  # Appliquer un thème minimal
  theme(
    axis.title.x = element_text(size = 8, color = "darkblue"), 
    axis.title.y = element_text(size = 8, color = "darkblue"),
    plot.subtitle = element_text(size = 10, color = "black")
  ) 

p6 <- ggplot(dataAnts, aes(sample = Elevation)) +
  labs(subtitle = "C. Quantile-Quantile Plot",
       caption = "D'après le jeu de données dataAnts",
       x = "Théorique Quantiles",
       y = "Quantiles Observés") +
  stat_qq() +  # Ajoute les points du Q-Q plot
  stat_qq_line(col = "red", size = 1) +  # Ajoute la ligne théorique
  theme_minimal() + # Applique un thème minimal
  theme(
    axis.title.x = element_text(size = 8, color = "darkblue"),
    axis.title.y = element_text(size = 8, color = "darkblue"),
    plot.title = element_text(size = 14, face = "bold", color = "black"),       
    plot.subtitle = element_text(size = 10, color = "black"),                  
    plot.caption = element_text(size = 4, face = "italic", color = "darkgrey") 
  ) 

(p4 / p5) | p6 + 
  plot_layout(guides = "collect")

```

On ne constate pas de valeurs aberrantes sur le Cleveland plot (Fig. 4A) ou sur l'histogramme (Fig. 4B).

On remarque grâce au Quantile plot (Fig. 4C) un écart à la normalité. Ceci suggère qu'il est ici interressant d'utilisé un modèle linéaire généralisé et non un modèle linéaire classique.

[**L'aire**]{.underline} **:**

```{r Area, echo=FALSE, message=FALSE, warning=FALSE, paged.print=TRUE}

p4 <- ggplot(dataAnts) +
  aes(x=Area, y=factor(1:nrow(dataAnts), levels = 1:nrow(dataAnts))) + # Affichage avec classification
  geom_jitter(width=0, height=0.1) +
  scale_x_continuous(breaks = seq(0, 865000, by = 200000)) + # Valeurs de l'axe X
  labs(title = "Figure 5 : Variable explicative Area",
       subtitle = "A. Cleveland Plot",
       x= "Area") +  # Titres
  theme_minimal() +
  theme(axis.text.y = element_blank(),   # Supprimer les étiquettes de l'axe Y
        axis.ticks.y = element_blank(),  # Supprimer les ticks de l'axe Y
        axis.title.y = element_blank(),  # Supprimer le titre de l'axe Y
        axis.title.x = element_text(size = 8, color = "darkblue"),
        plot.title = element_text(size = 14, face = "bold", color = "black"),
        plot.subtitle = element_text(size = 10, color = "black")
        )   

p5 <- ggplot(dataAnts) +
  aes(x = Area) +  # Utiliser Aera sur l'axe X
  geom_histogram(binwidth = 50000, fill = "blue", color = "black", alpha = 0.7) +  # Histogramme
  scale_x_continuous(breaks = seq(0, 865000, by = 200000)) +  # Valeurs de l'axe X
  scale_y_continuous(breaks = seq(0, 20, by = 5)) +  # Valeurs de l'axe X
  labs(subtitle = "B. Histogramme",
       x = "Area") +  # Titres
  theme_minimal() +  # Appliquer un thème minimal
  theme(
    axis.title.x = element_text(size = 8, color = "darkblue"), 
    axis.title.y = element_text(size = 8, color = "darkblue"),
    plot.subtitle = element_text(size = 10, color = "black")
  ) 

p6 <- ggplot(dataAnts, aes(sample = Area)) +
  labs(subtitle = "C. Quantile-Quantile Plot",
       caption = "D'après le jeu de données dataAnts",
       x = "Quantiles Théorique",
       y = "Quantiles Observés") +
  stat_qq() +  # Ajoute les points du Q-Q plot
  stat_qq_line(col = "red", size = 1) +  # Ajoute la ligne théorique
  theme_minimal() + # Applique un thème minimal
  theme(
    axis.title.x = element_text(size = 8, color = "darkblue"),
    axis.title.y = element_text(size = 8, color = "darkblue"),
    plot.title = element_text(size = 14, face = "bold", color = "black"),       
    plot.subtitle = element_text(size = 10, color = "black"),                  
    plot.caption = element_text(size = 4, face = "italic", color = "darkgrey") 
  ) 

(p4 / p5) | p6 + 
  plot_layout(guides = "collect")

```

Nous constatons des valeurs aberrantes sur le Cleveland plot (Fig. 5A) et sur l'histogramme (Fig. 5B) en raison de la présence de tourbières très étendues. Nous pouvons effectuer une transformation de cette variable et envisager une transformation logarithmique.

```{r LogArea, echo=FALSE, message=FALSE, warning=FALSE, paged.print=TRUE}

dataAnts$LogArea<-log(dataAnts$Area)

p4 <- ggplot(dataAnts) +
  aes(x=LogArea, y=factor(1:nrow(dataAnts), levels = 1:nrow(dataAnts))) + # Affichage avec classification
  geom_jitter(width=0, height=0.1) +
  scale_x_continuous(breaks = seq(2, 15, by = 2)) + # Valeurs de l'axe X
  labs(title = "Figure 6 : Variable explicative LogArea",
       subtitle = "A. Cleveland Plot",
       x= "LogArea") +  # Titres
  theme_minimal() +
  theme(axis.text.y = element_blank(),   # Supprimer les étiquettes de l'axe Y
        axis.ticks.y = element_blank(),  # Supprimer les ticks de l'axe Y
        axis.title.y = element_blank(),  # Supprimer le titre de l'axe Y
        axis.title.x = element_text(size = 8, color = "darkblue"),
        plot.title = element_text(size = 14, face = "bold", color = "black"),
        plot.subtitle = element_text(size = 10, color = "black")
        )   

p5 <- ggplot(dataAnts) +
  aes(x = LogArea) +  # Utiliser Aera sur l'axe X
  geom_histogram(binwidth = 2, fill = "blue", color = "black", alpha = 0.7) +  # Histogramme
  scale_x_continuous(breaks = seq(2, 15, by = 2)) +  # Valeurs de l'axe X
  scale_y_continuous(breaks = seq(0, 30, by = 5)) +  # Valeurs de l'axe X
  labs(subtitle = "B. Histogramme",
       x = "LogArea") +  # Titres
  theme_minimal() +  # Appliquer un thème minimal
  theme(
    axis.title.x = element_text(size = 8, color = "darkblue"), 
    axis.title.y = element_text(size = 8, color = "darkblue"),
    plot.subtitle = element_text(size = 10, color = "black")
  ) 

p6 <- ggplot(dataAnts, aes(sample = LogArea)) +
  labs(subtitle = "C. Quantile-Quantile Plot",
       caption = "D'après le jeu de données dataAnts",
       x = "Quantiles Théorique",
       y = "Quantiles Observés") +
  stat_qq() +  # Ajoute les points du Q-Q plot
  stat_qq_line(col = "red", size = 1) +  # Ajoute la ligne théorique
  theme_minimal() + # Applique un thème minimal
  theme(
    axis.title.x = element_text(size = 8, color = "darkblue"),
    axis.title.y = element_text(size = 8, color = "darkblue"),
    plot.title = element_text(size = 14, face = "bold", color = "black"),       
    plot.subtitle = element_text(size = 10, color = "black"),                  
    plot.caption = element_text(size = 4, face = "italic", color = "darkgrey") 
  ) 

(p4 / p5) | p6 + 
  plot_layout(guides = "collect")

```

On ne constate pas de valeurs aberrantes sur le Cleveland plot (Fig 6A) ou sur l'histogramme (Fig. 6B).

[**Conclusion**]{.underline} **:**

L'analyse de valeurs aberrantes et de la distribution des variables suggèrent qu'après modification les trois variables quantitatives (covariables) *Latitude*, *Elevation* et *LogArea.* peuvent être utilisées dans la construction du modèle.

**c.** $X$ qualitative : analyser le nombre de modalités et le nombre d'individus par modalité

Quand le jeu de données comporte des variables qualitatives (facteurs), il est nécessaire d'analyser l'équilibre des modalités: si il y a plus d'individus qui prennent une modalité par rapport à une autre, leur poids dans le modèle peuvent mener à interprétation finale biaisée.

```{r equilibre des modalites, echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}

summary(dataAnts$Location)

```

Ici les données sont bien réparties entre les deux modalités (Bog = tourbières et Forest = forêts).

[**Conclusion**]{.underline} **:**

L'analyse de valeurs aberrantes et de la distribution de la variable qualitative suggèrent qu'elle peut être utilisée dans la construction du modèle.

# ANALYSE DES INTERACTIONS ENTRE LES VARIABLES Y ET XS

-   **1.Analyser la relation potentielle entre Y et les Xs**

Afin de tester la relation éventuelle entre la variable réponse $Y$ et les variables explicatives $Xs$, il sera indispensable de réaliser la modélisation statistique, c'est le seul moyen qui nous permet de tester la significativité des relations. Cependant, nous pouvons commencer par réaliser des analyses graphiques afin de mieux visualiser les données et se faire une idée.

```{r relation Y et XS , echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}

par(mfrow=c(2,2))

# Pour la variable Latitude
plot(dataAnts$Nsp~dataAnts$Latitude,pch=16,col='deepskyblue4',xlab='Latitude',ylab='Richesse spécifique des fourmis', main = "Figure 7")

# Pour la variable Altitude
plot(dataAnts$Nsp~dataAnts$Elevation,pch=16,col='deepskyblue4',xlab='Altitude',ylab='Richesse spécifique des fourmis', main = "Figure 8")

# Pour la variable Superficie de la tourbière
plot(dataAnts$Nsp~dataAnts$LogArea,pch=16,col='deepskyblue4',xlab='Superficie de la tourbière',ylab='Richesse spécifique des fourmis', main = "Figure 9")

# Pour la variable Localisation
boxplot(dataAnts$Nsp~dataAnts$Location, varwidth = TRUE, ylab = "Richesse spécifique des fourmis", xlab = "Localisation", col=c('saddlebrown','palegreen4'), main = "Figure 10")

```

Relation entre $X$ et la latitude (Fig. 7) : Nous pouvons voir que plus la latitude est importante moins la richesse spécifique est élevée. Nous pouvons dire que plus nous nous trouvons au Nord, moins il y a d'espèces de fourmis.

Relation entre $X$ et l'altitude (Fig. 8) : Dans ce graphique-ci, nous pouvons voir à nouveau que plus l'altitude est importante, moins la richesse spécifique est élevée. Il semble que plus le relief est élévé moins il y a d'espèces de fourmis.

Relation entre $X$ et la surperficie de la tourbière (Fig. 9) : Ici, il ne semble pas y avoir de relation entre la richesse spécifique des fourmis et la surperficie des tourbières.

Relation entre $X$ et localisation (Fig. 10) : Il semble ici y avoir une relation entre le lieu et la richesse spécifique des fourmis, avec une richesse spécifique plus importante en forêt que dans les tourbières.

[**Conclusion**]{.underline} **:**

Il semble que plus les espèces se situent au nord et dans un haut relief, moins la richesse spécifique de fourmis est grande. De plus, si nous nous trouvons en forêt, il y aura une richesse spécifique plus importante que dans les tourbières.

-   **2.Analyse des éventuelles interactions entre les Xs**

Nous cherchons à étudier les interactions entre le facteur et les 3 covariables. Nous excluons alors l'interaction entre les variables explicatives quantitatives. Nous passons par une approche graphique afin d'estimer le possible effet interactif entre ces différentes variables.

```{r interactions Xs, echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}

par(mfrow=c(1,3))

# Interactions entre Latitude & Localisation
plot(dataAnts$Nsp~dataAnts$Latitude,type='n',ylab = "Richesse spécifique de fourmis",xlab="Latitude", main="Figure 11")
points(dataAnts$Nsp[dataAnts$Location=="Bog"]~dataAnts$Latitude[dataAnts$Location=="Bog"],pch=16,cex=2,col='saddlebrown')
points(dataAnts$Nsp[dataAnts$Location=="Forest"]~dataAnts$Latitude[dataAnts$Location=="Forest"],pch=17,cex=2,col='palegreen4')

# Interactions entre Altitude & Localisation
plot(dataAnts$Nsp~dataAnts$Elevation,type='n',ylab = "Richesse spécifique de fourmis",xlab="Altitude", main="Figure 12")
points(dataAnts$Nsp[dataAnts$Location=="Bog"]~dataAnts$Elevation[dataAnts$Location=="Bog"],pch=16,cex=2,col='saddlebrown')
points(dataAnts$Nsp[dataAnts$Location=="Forest"]~dataAnts$Elevation[dataAnts$Location=="Forest"],pch=17,cex=2,col='palegreen4')

# Interactions entre Superficie de la tourbière & Localisation
plot(dataAnts$Nsp~dataAnts$LogArea,type='n',ylab = "Richesse spécifique de fourmis",xlab="Superficie de la tourbière", main="Figure 13")
points(dataAnts$Nsp[dataAnts$Location=="Bog"]~dataAnts$LogArea[dataAnts$Location=="Bog"],pch=16,cex=2,col='saddlebrown')
points(dataAnts$Nsp[dataAnts$Location=="Forest"]~dataAnts$LogArea[dataAnts$Location=="Forest"],pch=17,cex=2,col='palegreen4')

```

En règle générale, nous pouvons penser qu'il y a une interaction entre les covariables si les pentes formées par les deux nuages de points sont différentes. Ici les nuages ne permettent pas de voir clairement de similitude ou de différence de tendance pour chaque graphique. Il est donc difficile de conclure sur ces interactions.

-   **3.Vérifier une éventuelle colinéarité entre les Xs**

Nous cherchons à éviter la colinéarité des variables explicatives dans notre modélisation, afin de ne pas avoir plusieurs variables qui donnent la même information. Pour ce faire, nous devons vérifier les liens statistiques qu'il peut y avoir entre les variables, \[1\] en calculant la corrélation entre les variables $Xs$ quantitatives, \[2\] puis en étudiant l'influence du $X$ qualitative sur les $Xs$ quantitatives à l'aide de graphiques plot.

```{r colinéarité1, echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}

# Nous vérifions la colinéarité entre les variables continues indépendantes
# Nous faisons un graphique pour chaque paire de covariables
newdata<-cbind(dataAnts$Latitude,dataAnts$Elevation,dataAnts$LogArea)
colnames(newdata)<-c('Latitude','Altitude','Log Superficie tourbière')
newdata<-data.frame(newdata)
plot(newdata,pch=16,col='deepskyblue4',main="Figure 14")

# On calcule la corrélation de chaque paire de covariables
M<-cor(newdata)
corrplot.mixed(M,upper="square",lower.col="black", tl.col="black",cl.cex = 0.8,tl.cex = 0.7,number.cex =0.8)
mtext("Figure 15", side = 2, line = -6, cex = 1, las = 1)

par(mfrow=c(1,3))

# Nous vérifions la colinéarité entre catégories et variables continues
# Latitude et Localisation
boxplot(dataAnts$Latitude~dataAnts$Location, varwidth = TRUE, ylab = "Latitude", xlab = "Localisation",col=c('saddlebrown','palegreen4'), main = "Figure 16")

# Altitude et Localisation
boxplot(dataAnts$Elevation~dataAnts$Location, varwidth = TRUE, ylab = "Altitude", xlab = "Localisation",col=c('saddlebrown','palegreen4'), main = "Figure 17")

# Superficie de la tourbière et Localisation
boxplot(dataAnts$LogArea~dataAnts$Location, varwidth = TRUE, ylab = "Superficie de la tourbière", xlab = "Localisation",col=c('saddlebrown','palegreen4'), main = "Figure 18")

```

Nous pouvons observer sur le premier graphique (Fig. 14) que les nuages de points ne semblent pas montrer de tendance à la colinéarité. Cette impression est confirmée par les valeurs calculées dans la figure suivante (Fig. 15). Dans cette dernière, les valeurs sont largement inférieures à 0.7, qui est très souvent utilisé comme seuil de colinéarité (au-delà les variables sont considérées corrélées). Le chevauchement presque total des boxplot (Fig. 16, 17, 18) confirme encore une fois la non colinéarité des facteurs.

[**Conclusion**]{.underline} **:**

Il ne semble pas y avoir de colinéarité entre les variables, nous pouvons donc toutes les conserver.

# ANALYSE STATISTIQUE

-   **1.Construction du modèle**

Nous cherchons à produire le meilleur modèle pour prédire le nombre d'espèces. La variable réponse est un comptage (variable quantitative discrète) de petit effectifs (\<30), la loi appropriée est donc la loi de **Poisson**. Nous allons donc réaliser la modélisation avec un modèle linéaire généralisé de Poisson.

Pour arriver au modèle, dit "candidat", à tester, nous allons partir du modèle dit "complet", c'est-à-dire avec toutes les variables explicatives et leurs interactions, et utiliser une méthode pas à pas arrière ou **Backward selection** pour sélectionner les variables et les interactions à conserver.

Commençons avec le modèle complet:

```{r, echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}

# Formulation du modèle avec la fonction glm() et family=poisson(link="log")
mod1<-glm(Nsp~ Location
        + Latitude
        + Elevation
        + LogArea
        + Location:Latitude
        + Location:Elevation
        + Location:LogArea
        ,data=dataAnts
        ,family=poisson(link="log"))

```

A partir de ce modèle complet nous allons éliminer pas à pas les interactions non significatives.

```{r colinéarité2, echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}

# Etape 1
drop1(mod1,test="Chi")

```

Les trois interactions ne sont pas significatives. Nous enlèvons l'interaction 'Location:Latitude' car c'est la 'moins significative'. Nous pouvons également remarquer que la fonction drop1() nous informe que c'est en enlevant cette interaction que nous obtenons le meilleur AIC.

Nous obtenons donc le modèle suivant:

```{r, echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}

# Etape 2
mod1<-glm(Nsp~ Location
        + Latitude
        + Elevation
        + LogArea
        + Location:Elevation
        + Location:LogArea
        ,data=dataAnts
        ,family=poisson(link="log"))

drop1(mod1,test="Chi")

```

De la même façon, nous enlèvons l'interaction 'Location:LogArea'.

```{r, echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}

# Etape 3
mod1<-glm(Nsp~ Location
        + Latitude
        + Elevation
        + LogArea
        + Location:Elevation
        ,data=dataAnts
        ,family=poisson(link="log"))

drop1(mod1,test="Chi")

```

Nous enlèvons également l'interaction 'Location:Elevation'. Nous regardons maintenant la significativité des variables explicatives.

```{r, echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}

# Etape 4
mod1<-glm(Nsp~ Location
        + Latitude
        + Elevation
        + LogArea
        ,data=dataAnts
        ,family=poisson(link="log"))

drop1(mod1,test="Chi")

```

La variable 'LogArea' n'a pas un effet siginificatif, nous l'éliminons du modèle.

```{r, echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}

# Etape 5
mod1<-glm(Nsp~ Location
        + Latitude
        + Elevation
        ,data=dataAnts
        ,family=poisson(link="log"))

drop1(mod1,test="Chi")

```

Les variables explicatives restantes sont significatives, nous conservons ainsi ce modèle comme modèle candidat.

Nous allons ensuite identifier le modèle candidat avec une autre approche que la significativité des p-values : l'AIC avec le mode stepwise.

```{r model AIC, echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}

# Nous construisons le modèle complet
modelcomplet<- glm(Nsp~ Location
        + Latitude
        + Elevation
        + LogArea
        + Location:Latitude
        + Location:Elevation
        + Location:LogArea
        ,data=dataAnts
        ,family=poisson(link="log"))

# Sélection du modèle sur la base AIC
modfinal<-stepAIC(modelcomplet,direction="both")

# Analyse du modèle final
drop1(modfinal,test="Chi")
summary(modfinal)

```

Remarque : il est recommandé d'utiliser l'AIC avec plus de 40 individus par paramètre, sinon il existe l'AICc (@YannickOutreman).

[**Conclusion**]{.underline} **:**

Les deux méthodes arrivent au même modèle candidat : *Nsp\~Latitude+Elevation+LogArea*.

-   **2.Analyse des coefficients**

L'étape suivante est l'analyse des coefficients. Nous allons donc regarder l'effet précis des variables explicatives retenues (Latitude, Elevation, LogArea) sur la richesse spécifique des fourmis.

```{r, echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}

# Coefficients du modèle
summary(mod1)
#                 Estimate Std. Error z value Pr(>|z|)    
#(Intercept)    11.9368121  2.6214970   4.553 5.28e-06 ***
#LocationForest  0.6354389  0.1195664   5.315 1.07e-07 ***
#Latitude       -0.2357930  0.0616638  -3.824 0.000131 ***
#Elevation      -0.0011411  0.0003749  -3.044 0.002337 ** 

#Null deviance: 102.76  on 43  degrees of freedom
#Residual deviance:  40.69  on 40  degrees of freedom

```

Le summary() du modèle nous permet de voir les coefficients correspondant à chaque variable ainsi que l'intercept (basé sur la moyenne de la population). Pour un facteur (variable qualitative) le coefficient correspond à la comparaison entre la modalité donnée et la modalité dite "de référence" dont le coefficient est égal à 0. Les scores de déviance serviront dans la partie suivante. Grâce à ce tableau nous pouvons noter les différents coefficients :

**Location factor**\
- $Location_{Bog}$ = 0 (modalité de référence du facteur Location)\
- $Location_{Forest}$ = $+0.63^{**}$\
**Latitude covariate**\
- $\beta_{Latitude}$ = $-0.23^{***}$\
**Elevation covariate**\
- $\beta_{Elevation}$ = $-0.001^{***}$

Le modèle candidat avec les coefficients s'écrit (équation 5) : $$ log(Species\:Richness) = 11.93^{***} + (Location_{Bog} = 0 ;\:Location_{Forest} = +0.63^{***})\:- 0.23^{***}.Latitude\: -0.001^{***}. Elevation  $$ {#eq-5}

Rappel : pour modéliser une variable quantitative discrète avec un modèle linéaire généralisé nous passons par une fonction de lien, ici avec la loi de Poisson nous utilisons la fonction log().

-   **3.Pouvoir explicatif du modèle**

Afin de connaître la qualité de notre modèle, nous allons calculer la distance entre le modèle candidat et le modèle nul. Le modèle nul est un modèle qui résume les données avec un unique paramètre : la moyenne de $Y$. Ce modèle n'explique pas les données.

Un moyen de connaître la qualité du modèle est de calculer un *pseudo R²*. Pour cela, nous déterminons la distance entre la déviance du modèle nul et la déviance résiduelle du modèle candidat (équation 6).

$$Pseudo\:R^2=100\:.\:\frac{Déviance\:nulle- Déviance\:résiduelle}{Déviance\:nulle}$$ {#eq-6}

Nous pouvons calculer le *pseudo R²* de trois manières différentes : avec la méthode de McFadden, celle de Cox et Snell ou celle de Nagelkerke. Les deux dernières méthodes nécessitent le package 'rcompagnon'.

Dans notre exemple, comme vu dans la partie précédente, le modèle nul a une déviance de 102,76 et la déviance résiduelle est de 40,69.

Nous allons donc calculer la qualité du modèle avec les commandes suivantes :

```{r deviance, echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}

# Estimate of deviance explained
(mod1$null.deviance-mod1$deviance)/mod1$null.deviance

# Some others estimates of deviance explained - package 'rcompanion'
library(rcompanion)
nagelkerke(mod1)

```

Ici, l'estimation de la déviance expliquée est de 60% ou 75%, en fonction de la méthode utilisée.


```{r Prediction,include=TRUE, fig.height=6, fig.width=6}

plot(fitted(mod1)~dataAnts$Nsp,col='dodgerblue3',pch=16,ylab="Valeurs estimées par le modèle",xlab="Valeurs observées", main="Figure 19")
abline(0,1)

```

-   **4.Validation du modèle**

L'*indépendance des résidus* est la seule chose qui limite le modèle linéaire généralisé. L'analyse des résidus permet donc d'identifier d'éventuelles tendances et de vérifier la présence d'unités statistiques influentes.

Dans un premier temps, vérifier la présence de **surdispersion** est indispensable lorsque l'on fait un modèle linéaire généralisé suivant une loi de Poisson. Pour tous les modèles linéaires généralisés utilisant des données de comptage, il faut vérifier l'absence de surdispersion.

Pour cela, nous allons calculer un paramètre appelé 'scale parameter'. Si sa valeur est nettement supérieure à 1 (à partir de 1.6 ou 1.7), il y a surdispersion. Le modèle n'est donc pas valable.

**a.Vérification de la surdispersion**

Quand l'indice de dispersion est supérieur à 1.5, il existe différentes explications.

Cela peut être dû à :

-   Des valeurs abérrantes sont présentes dans le jeu de données. Il faut donc les éliminer.

-   Des covariables sont manquantes. Il faut les ajouter au modèle.

-   Des interactions sont manquantes. Il faut les ajouter au modèle.

-   Une inflation des zéros se produit. Il faut utiliser une regression Binomiale négative à inflation des zéros. (@Long_J.S._1997)

-   Une dépendance entre les données et un facteur ou une variable est présente. Il faut utiliser un modèle linéaire général mixte. (@Johnson_et_al._2015)

-   Relation entre les données et les variables ou facteurs n'est pas linéaire. Il faut utiliser un modèle additif généralisé. (@Irz_et_Levi-Valensin_2016)

-   La fonction de lien est mauvaise. Il faut changer la fonction de lien.

-   La variation de $Y$ est grande. Il faut utiliser un modèle linéaire généralisé suivant une loi binomiale négative. (@Chahine_1965)

Dans notre exemple, nous allons calculer le 'scale parameter'. Pour cela nous allons utiliser la commande :

```{r surdispersion, echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}

# Scale parameter calculation
E1 <- resid(mod1, type = "pearson") # (Y - mu) / sqrt(mu)
N  <- nrow(dataAnts)
p  <- length(coef(mod1))
sum(E1^2) / (N - p)

```

[**Conclusion**]{.underline} **:**

Ici, la dispersion est de 1.02. Il n'y a pas de surdispersion.

**b.Analyses des résidus**

Si les hypothèses de normalité et d'homogénéité des résidus ne sont pas attendues dans les modèles linéaires généralisés, nous pouvons cependant les analyser graphiquement. En traçant les résidus en fonction des $Xs$ significatifs du modèle, nous devons valider l'absence de tendance dans leur distribution. Si nous constatons une tendance, la modélisation peut être problématique : cela peut être dû à un manque d'ajustement, à une dépendance dans les données ou à des observations influentes. Rappelons que dans les modèles linéaires généralisés, nous utilisons **les résidus de Pearson** car ils incluent l'hétérogénéité de la variance et sont faciles à calculer et à comprendre.

```{r, echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}

resid<-residuals(mod1, type="pearson")

par(mfrow=c(3,2))
# Histogram
hist(resid,col='dodgerblue3',xlab="residuals",main="Figure 20")
# Quantile-Quantile plot
qqnorm(resid,pch=16,col='dodgerblue3',xlab='', main="Figure 21")
qqline(resid,col='red',lwd=2)

# Residuals vs fitted
plot(resid~fitted(mod1)
      , col='dodgerblue3'
      , pch=16, main="Figure 22")
abline(h = 0)

# Residuals against Location factor
boxplot(resid~ dataAnts$Location, 
         varwidth = TRUE,
         ylab = "Residuals",
         xlab = "Location",
         main = "Figure 23")

# Residuals against Latitude
plot(resid~ dataAnts$Latitude, 
         pch=16,
         col="dodgerblue3",
         ylab = "Residuals",
         xlab = "Latitude",
         main = "Figure 24")
abline(h = 0)

# Residuals against Elevation
plot(resid~ dataAnts$Elevation, 
         pch=16,
         col="dodgerblue3",
         ylab = "Residuals",
         xlab = "Elevation",
         main = "Figure 25")
abline(h = 0)

```

L'histogramme des résidus (Fig. 20) et le Q-Q plot (Fig. 21) montrent des résidus relativement normaux. Les quatres graphiques suivant (Fig. 22, 23, 24, 25) montrent la distribution les résidus selon les estimations du modèle et les trois variables explicatives. Nous pouvons voir que les résidus restent homogènes peu importe les valeurs des variables explicatives.

[**Conclusion**]{.underline} **:**

Nous pouvons conclure qu'il n'y a pas de tendance dans les résidus.

**c.Vérification des individus influents**

```{r contri, echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}

par(mfrow = c(1, 1))
plot(cooks.distance(mod1), type = "h", ylim = c(0, 1), main="Figure 26")
abline(h = 1, col = 2,lwd = 3)

```

[**Conclusion**]{.underline} **:**

Ici (Fig. 26), il n'y a pas d'individus trop influent.

**d.Analyses des résidus avec DHARMa**

Pour une représentation graphique différente, nous pouvons utiliser le package 'DHARMa'. Ce package permet d'analyser les résidus et plus particulièrement leur surdispersion et les valeurs aberrantes. Ce package met en couleur rouge les résidus ne respectant pas les valeurs attendues.

```{r Dharma, echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}

# TestDispersion(mod1)
library(DHARMa)
simulationOutput <- simulateResiduals(fittedModel = mod1, plot = F)
residuals(simulationOutput)
plot(simulationOutput)
residuals(simulationOutput, quantileFunction = qnorm, outlierValues = c(-7,7))

# Représentation des résidus selon la covariable
plotResiduals(simulationOutput, form = dataAnts$Latitude)

```

[**Conclusion**]{.underline} **:**

Dans notre exemple, les résidus ne sont pas surdispersés et ne montrent pas de valeurs aberrantes.

# CONCLUSION

Le modèle que nous retenons est le suivant (équation 5) :

$$ log(Species\:Richness) = 11.93^{***} + (Location_{Bog} = 0 ;\:Location_{Forest} = +0.63^{***})\:- 0.23^{***}.Latitude\: -0.001^{***}. Elevation  $$

Son index de dispersion est estimé à 1.02 ce qui montre une absence de surdispersion permettant de valider notre modèle.

De même, nous ne détectons pas la présence d'individus influents pouvant biaiser le modèle.

Le calcul du pseudo R² estimé à 0.60 nous permet de considérer que ce modèle explique bien nos données (la déviance du modèle nul étant de 102.76 et la déviance résiduelle de 40.69).

Nous en concluons donc que la richesse spécifique des fourmis dans les tourbières du New-England dépend :

-   de la localisation : la richesse spécifique est plus forte en forêt qu'en tourbière en lien avec le couvert végétal et la lumière disponible ;

-   de la latitude : la richesse spécifique diminue avec la latitude ;

-   et de l'altitude, bien que l'effet de celle-ci soit moindre.

Dans les forêts comme dans les tourbières, la latitude est le prédicteur le plus significatif de la densité des espèces.

Cependant, d'autres recherches à plus grandes échelles sont nécessaires pour généraliser ces résultats, l'étude ne couvrant qu'une zone géographique restreinte avec deux types d'habitats seulement.
